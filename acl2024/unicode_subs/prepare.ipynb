{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffac894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import json\n",
    "import my_datasets\n",
    "from unicode_substitutions import replace_all, sample_substitution, selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769d6a85",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "out_dataset_name = \"data/frac:2/wikitext_perturbed\"\n",
    "out_samples_name = \"data/frac:2/samples.csv\"\n",
    "\n",
    "ds_name = 'pile_100M'\n",
    "strategy = 'sample_chars'\n",
    "\n",
    "seed = 0\n",
    "num_samples = 200\n",
    "num_proc = 16\n",
    "debug = False\n",
    "\n",
    "frac_controlled = 1.0\n",
    "frac_contaminated = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9861148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ryan/haveibeentrainedon/data/pile1e8_orig/cache-db4e2c3fb658967d.arrow\n"
     ]
    }
   ],
   "source": [
    "ds = my_datasets.get_dataset(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358e783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_idx = int(frac_controlled * 0.01 * len(ds))\n",
    "control_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5902582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2879"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's possible that we are perturbing duplicated sequences\n",
    "contaminated_idx = int(frac_contaminated * 0.01 * len(ds))\n",
    "contaminated_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475f00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ra(text, seed=seed):\n",
    "    words = text.split(' ')\n",
    "    words = [ replace_all(word, seed) if word.isalnum() else word for word in words ]\n",
    "    total = sum([ 1 if w != w_ else 0 for w, w_ in zip(text.split(' '), words)])\n",
    "    text = ' '.join(words)\n",
    "    return total, text\n",
    "\n",
    "def sc(text, seed=seed):\n",
    "    words = text.split(' ')\n",
    "    words = [ sample_substitution(word, seed) if word.isalnum() else word for word in words ]\n",
    "    total = sum([ 1 if w != w_ else 0 for w, w_ in zip(text.split(' '), words)])\n",
    "    text = ' '.join(words)\n",
    "    return total, text\n",
    "\n",
    "if strategy == 'replace_all':\n",
    "    perturb = ra\n",
    "elif strategy == 'sample_chars':\n",
    "    perturb = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f84e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ryan/haveibeentrainedon/data/pile1e8_orig/cache-14c333ce47a93a61.arrow\n"
     ]
    }
   ],
   "source": [
    "edited_ds = ds.add_column('bits', [0]*len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e6f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purposes\n",
    "if debug:\n",
    "    edited_ds = edited_ds.select(range(control_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c795d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/57586 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Performs the map that will perturb the data. Records the perturbation in the \"order\" section of the data\n",
    "def edit(x, index):\n",
    "    order = []\n",
    "    if index >= contaminated_idx:\n",
    "        return x\n",
    "    \n",
    "    text = x['text']\n",
    "    \n",
    "    # different seed for each \"player\", up to 32 players\n",
    "    total, text = perturb(text, seed=int(index / control_idx))\n",
    "    \n",
    "    if total != 0:\n",
    "        assert(x['text'] != text)\n",
    "        \n",
    "    x[\"text\"] = text\n",
    "    x[\"bits\"] = total\n",
    "    return x\n",
    "\n",
    "edited_ds = edited_ds.map(\n",
    "    edit,\n",
    "    num_proc=num_proc,\n",
    "    with_indices=True,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe77371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# edited_ds[0]['text'], json.dumps(edited_ds[0]['text'])\n",
    "# np.mean([ selected('aloha', i) for i in range(0,1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e456d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "seeds = np.random.randint(0, 10000, size=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8b2565",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m data\u001b[38;5;241m.\u001b[39mappend([i, edited_ds[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mTrue\u001b[39;00m, edited_ds[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[0;32m---> 10\u001b[0m     total, perturbed_text \u001b[38;5;241m=\u001b[39m \u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend([i, perturbed_text, \u001b[38;5;28;01mFalse\u001b[39;00m, total])\n",
      "Cell \u001b[0;32mIn [6], line 10\u001b[0m, in \u001b[0;36msc\u001b[0;34m(text, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msc\u001b[39m(text, seed\u001b[38;5;241m=\u001b[39mseed):\n\u001b[1;32m      9\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     words \u001b[38;5;241m=\u001b[39m [ sample_substitution(word, seed) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;28;01melse\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words ]\n\u001b[1;32m     11\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([ \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m!=\u001b[39m w_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w, w_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), words)])\n\u001b[1;32m     12\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "Cell \u001b[0;32mIn [6], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msc\u001b[39m(text, seed\u001b[38;5;241m=\u001b[39mseed):\n\u001b[1;32m      9\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     words \u001b[38;5;241m=\u001b[39m [ \u001b[43msample_substitution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;28;01melse\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words ]\n\u001b[1;32m     11\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([ \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;241m!=\u001b[39m w_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w, w_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m), words)])\n\u001b[1;32m     12\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n",
      "File \u001b[0;32m~/gpt-neox/haveibeentrainedon/acl2024/unicode_subs/unicode_substitutions.py:45\u001b[0m, in \u001b[0;36msample_substitution\u001b[0;34m(x, seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m.\u001b[39mdigest(), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m---> 45\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchar_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m masked_dict \u001b[38;5;241m=\u001b[39m { i:j \u001b[38;5;28;01mfor\u001b[39;00m (i, j), m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(char_dict\u001b[38;5;241m.\u001b[39mitems(), mask) \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m }\n\u001b[1;32m     48\u001b[0m substitute \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([ masked_dict\u001b[38;5;241m.\u001b[39mget(c, c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m x ])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(control_idx):\n",
    "    \n",
    "    if edited_ds[i]['bits'] < 10:\n",
    "        continue\n",
    "    \n",
    "    data.append([i, edited_ds[i]['text'], True, edited_ds[i]['bits']])\n",
    "\n",
    "    for s in seeds:\n",
    "        total, perturbed_text = perturb(ds[i]['text'], seed=s)\n",
    "        data.append([i, perturbed_text, False, total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b251915",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_inputs = pd.DataFrame(data)\n",
    "prop_inputs.columns = ['group', 'watermark', 'used?', 'bits']\n",
    "prop_inputs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_inputs.to_csv(out_samples_name, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_ds.save_to_disk(f'{out_dataset_name}.hf')\n",
    "edited_ds = datasets.load_from_disk(f'{out_dataset_name}.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d273981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the data\n",
    "# edited_ds.remove_columns(['hash', 'is_original', 'substitutions'])\n",
    "edited_ds.to_json(f'{out_dataset_name}.jsonl', num_proc=num_proc)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
