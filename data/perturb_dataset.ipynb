{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffac894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from substitutions import tenk_word_pairs as word_pairs\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769d6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = \"17e7_tokens.jsonl\"\n",
    "out_dataset_name = \"17e7_tokens_perturbed\"\n",
    "n_per_sub = 1000\n",
    "num_proc = 16\n",
    "seed = 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9861148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/johnny/.cache/huggingface/datasets/json/default-45df1c8a959db879/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094655b9c1034f7883c9c4af1529ab1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'meta'],\n",
       "        num_rows: 989378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This converts the jsonl to huggingface\n",
    "ds = datasets.load_dataset(\"json\", data_files=orig_data)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe4fc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This appends a \"hash\" column to each entry\n",
    "def get_duplicated(entry, idx):\n",
    "    hash_val = hash(entry[\"text\"])\n",
    "    entry[\"hash\"] = hash_val\n",
    "    return entry\n",
    "\n",
    "ds = ds[\"train\"].map(get_duplicated, with_indices=True, num_proc=num_proc, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f158648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of hash counter = 986474\n"
     ]
    }
   ],
   "source": [
    "# This creates a counter for the hashes\n",
    "hash_counter = Counter(ds[\"hash\"])\n",
    "print(f\"length of hash counter = {len(hash_counter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf845a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2693c0d6104dc8a5401b15a775c67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# appends a column that represents whether or not the data is duplicated\n",
    "def append_duplicated_column(entry):\n",
    "    entry[\"is_original\"] = (hash_counter[entry[\"hash\"]] == 1)\n",
    "    return entry\n",
    "\n",
    "ds = ds.map(append_duplicated_column, num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863aa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_counter = Counter(ds[\"is_original\"])\n",
    "print(f\"is_original counter = {duplicated_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4311fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels unique sentences with corresponding word pairs\n",
    "def label(x):\n",
    "    # compute corresponding label matrix\n",
    "    if x[\"is_original\"]:\n",
    "        labels = [1 if f' {i} ' in x['text'] else 0 for i, _ in word_pairs]\n",
    "        x['substitutions'] = labels\n",
    "        return x\n",
    "    # dont consider duplicated documents, so set all to 0\n",
    "    else:\n",
    "        x[\"substitutions\"] = [0 for i in range(len(word_pairs))]\n",
    "        return x\n",
    "\n",
    "ds = ds.map(label, num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf32e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_arr = np.array(ds[\"substitutions\"])\n",
    "print(swap_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This random state allows the perturbations to be reproducible\n",
    "rs = np.random.RandomState(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed7096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used for keeping track of which words have been perturbed\n",
    "ds = ds.add_column('order', [''] * len(ds))\n",
    "edited_ds = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1221c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the sequences to perturb\n",
    "do_sub = []\n",
    "for i, (w1, w2) in tqdm(enumerate(word_pairs), total=len(word_pairs)):\n",
    "    # create indices\n",
    "    idx = np.arange(len(swap_arr))\n",
    "    has_sub = idx[swap_arr[:, i] == 1]\n",
    "    rs.shuffle(has_sub)\n",
    "    \n",
    "    all_indexes = has_sub[:n_per_sub]\n",
    "    labels = rs.randint(0, 2, size=n_per_sub).astype(bool)\n",
    "    do_sub.append(all_indexes[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66190962",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([ i.sum() for i in do_sub ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5783bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(do_sub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs the map that will perturb the data. Records the perturbation in the \"order\" section of the data\n",
    "def edit(x, index):\n",
    "    order = []\n",
    "    for i, (w1, w2) in enumerate(word_pairs):\n",
    "        if index not in do_sub[i]:\n",
    "            continue\n",
    "        \n",
    "        w1_index = x['text'].rindex(f' {w1} ')\n",
    "        order.append((i, w1_index))\n",
    "        \n",
    "        new_text = f' {w2} '.join(x['text'].rsplit(f' {w1} ', 1))\n",
    "        \n",
    "        assert (new_text != x['text'])\n",
    "        x[\"text\"] = new_text\n",
    "    \n",
    "    x[\"order\"] = json.dumps(order)\n",
    "    return x\n",
    "\n",
    "edited_ds = edited_ds.map(\n",
    "    edit,\n",
    "    num_proc=num_proc,\n",
    "    with_indices=True,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_ds.save_to_disk(f'{out_dataset_name}.hf')\n",
    "edited_ds = datasets.load_from_disk(f'{out_dataset_name}.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d273981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the data\n",
    "edited_ds.remove_columns(['hash', 'is_original', 'substitutions'])\n",
    "edited_ds.to_json(f'{out_dataset_name}.jsonl', num_proc=num_proc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
