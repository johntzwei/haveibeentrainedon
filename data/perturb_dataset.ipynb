{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffac894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from substitutions import tenk_word_pairs as word_pairs\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769d6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = \"17e7_tokens.jsonl\"\n",
    "out_dataset_name = \"17e7_tokens_perturbed\"\n",
    "n_per_sub = 500\n",
    "num_proc = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9861148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/johnny/.cache/huggingface/datasets/json/default-4a698c78fbd48a2e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c654d53b9e43d0b30585374f2c54ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'meta'],\n",
       "        num_rows: 989378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This converts the jsonl to huggingface\n",
    "ds = datasets.load_dataset(\"json\", data_files=orig_data)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe4fc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This appends a \"hash\" column to each entry\n",
    "def get_duplicated(entry, idx):\n",
    "    hash_val = hash(entry[\"text\"])\n",
    "    entry[\"hash\"] = hash_val\n",
    "    return entry\n",
    "\n",
    "ds = ds[\"train\"].map(get_duplicated, with_indices=True, num_proc=num_proc, keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f158648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of hash counter = 986474\n"
     ]
    }
   ],
   "source": [
    "# This creates a counter for the hashes\n",
    "hash_counter = Counter(ds[\"hash\"])\n",
    "print(f\"length of hash counter = {len(hash_counter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf845a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# appends a column that represents whether or not the data is duplicated\n",
    "def append_duplicated_column(entry):\n",
    "    entry[\"is_original\"] = (hash_counter[entry[\"hash\"]] == 1)\n",
    "    return entry\n",
    "\n",
    "ds = ds.map(append_duplicated_column, num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df93de48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', 'hash', 'is_original'],\n",
       "    num_rows: 989378\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863aa90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_original counter = Counter({True: 983916, False: 5462})\n"
     ]
    }
   ],
   "source": [
    "duplicated_counter = Counter(ds[\"is_original\"])\n",
    "print(f\"is_original counter = {duplicated_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4311fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labels unique sentences with corresponding word pairs\n",
    "def label(x):\n",
    "    # compute corresponding label matrix\n",
    "    if x[\"is_original\"]:\n",
    "        labels = [1 if f' {i} ' in x['text'] else 0 for i, _ in word_pairs]\n",
    "        x['substitutions'] = labels\n",
    "        return x\n",
    "    # dont consider duplicated documents, so set all to 0\n",
    "    else:\n",
    "        x[\"substitutions\"] = [0 for i in range(len(word_pairs))]\n",
    "        return x\n",
    "\n",
    "ds = ds.map(label, num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf32e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(989378, 45)\n"
     ]
    }
   ],
   "source": [
    "swap_arr = np.array(ds[\"substitutions\"])\n",
    "print(swap_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dbf342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This random state allows the perturbations to be reproducible\n",
    "rs = np.random.RandomState(seed=416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceed7096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The table can't have duplicated columns but columns ['order'] are duplicated.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# used for keeping track of which words have been perturbed\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_column\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43morder\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m edited_ds \u001B[38;5;241m=\u001B[39m ds\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/datasets/arrow_dataset.py:545\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    538\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    539\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    540\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    543\u001B[0m }\n\u001B[1;32m    544\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 545\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    546\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    547\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/datasets/fingerprint.py:511\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    507\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[1;32m    509\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[0;32m--> 511\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/datasets/arrow_dataset.py:5530\u001B[0m, in \u001B[0;36mDataset.add_column\u001B[0;34m(self, name, column, new_fingerprint)\u001B[0m\n\u001B[1;32m   5503\u001B[0m \u001B[38;5;124;03m\"\"\"Add column to Dataset.\u001B[39;00m\n\u001B[1;32m   5504\u001B[0m \n\u001B[1;32m   5505\u001B[0m \u001B[38;5;124;03m<Added version=\"1.7\"/>\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5527\u001B[0m \u001B[38;5;124;03m```\u001B[39;00m\n\u001B[1;32m   5528\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5529\u001B[0m column_table \u001B[38;5;241m=\u001B[39m InMemoryTable\u001B[38;5;241m.\u001B[39mfrom_pydict({name: column})\n\u001B[0;32m-> 5530\u001B[0m \u001B[43m_check_column_names\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcolumn_table\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5531\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatten_indices() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   5532\u001B[0m \u001B[38;5;66;03m# Concatenate tables horizontally\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/datasets/arrow_dataset.py:632\u001B[0m, in \u001B[0;36m_check_column_names\u001B[0;34m(column_names)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(count \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m count \u001B[38;5;129;01min\u001B[39;00m counter\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    631\u001B[0m     duplicated_columns \u001B[38;5;241m=\u001B[39m [col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m counter \u001B[38;5;28;01mif\u001B[39;00m counter[col] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 632\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe table can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have duplicated columns but columns \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mduplicated_columns\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m are duplicated.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: The table can't have duplicated columns but columns ['order'] are duplicated."
     ]
    }
   ],
   "source": [
    "# used for keeping track of which words have been perturbed\n",
    "ds = ds.add_column('order', [''] * len(ds))\n",
    "edited_ds = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1221c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc37175ac77847a9909eef3290b58d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take the sequences to perturb\n",
    "do_sub = []\n",
    "for i, (w1, w2) in tqdm(enumerate(word_pairs), total=len(word_pairs)):\n",
    "    # create indices\n",
    "    idx = np.arange(len(swap_arr))\n",
    "    has_sub = idx[swap_arr[:, i] == 1]\n",
    "    rs.shuffle(has_sub)\n",
    "    do_sub.append(list(has_sub[:n_per_sub]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66190962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11086067423"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(do_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5783bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(do_sub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050e5530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Performs the map that will perturb the data. Records the perturbation in the \"order\" section of the data\n",
    "def edit(x, index):\n",
    "    for i, (w1, w2) in enumerate(word_pairs):\n",
    "        if index not in do_sub[i]:\n",
    "            continue\n",
    "        order = x['order'] + f'{i}:'\n",
    "        new_text = x['text'].replace(f' {w1} ', f' {w2} ', 1)\n",
    "        assert (new_text != x['text'])\n",
    "        x[\"text\"] = new_text\n",
    "        x[\"order\"] = order\n",
    "    return x\n",
    "\n",
    "edited_ds = edited_ds.map(\n",
    "    edit,\n",
    "    num_proc=num_proc,\n",
    "    with_indices=True,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e48a86e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/14 shards):   0%|          | 0/989378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edited_ds.save_to_disk(f'{out_dataset_name}.hf')\n",
    "edited_ds = datasets.load_from_disk(f'{out_dataset_name}.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d273981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a3a60b54954758aa7730792c2f9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/990 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6599259815"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saves the data\n",
    "edited_ds.remove_columns(['hash', 'is_original', 'substitutions'])\n",
    "edited_ds.to_json(f'{out_dataset_name}.jsonl', num_proc=num_proc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
